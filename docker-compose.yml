services:
  api:
    build: .
    ports:
      - "3005:3000"
    volumes:
      - ./models:/app/models
      - ./.cache:/root/.cache
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - NODE_ENV=production
      - VLLM_IMAGE=${VLLM_IMAGE:-vllm/vllm-openai:latest}
      - VLLM_MODEL=${VLLM_MODEL:-mistralai/Mistral-7B-Instruct-v0.3}
      - VLLM_PORT=${VLLM_PORT:-8000}
      - VLLM_CONTAINER=${VLLM_CONTAINER:-vllm-openai}
      - VLLM_USE_GPU=${VLLM_USE_GPU:-}
      - PORT=${PORT:-3000}
    networks:
      - vllm-network

  vllm:
    image: ${VLLM_IMAGE:-vllm/vllm-openai:latest}
    ports:
      - "${VLLM_PORT:-8000}:8000"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
    volumes:
      - ./models:/root/.cache/huggingface/hub
    command: --model ${VLLM_MODEL:-mistralai/Mistral-7B-Instruct-v0.3}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - vllm-network
    profiles:
      - gpu

networks:
  vllm-network:
    driver: bridge

volumes:
  models:
